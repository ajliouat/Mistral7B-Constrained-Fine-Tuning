{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRVp0h2AnT9ePEB86TfWkI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajliouat/Mistral7B-Constrained-Fine-Tuning/blob/main/Mistral7b_Constrained_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv92RP3YquYM",
        "outputId": "3420df6d-469e-4066-8e97-2cfae25cf6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mistral-finetune'...\n",
            "remote: Enumerating objects: 364, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (156/156), done.\u001b[K\n",
            "remote: Total 364 (delta 122), reused 132 (delta 69), pack-reused 139\u001b[K\n",
            "Receiving objects: 100% (364/364), 258.41 KiB | 13.60 MiB/s, done.\n",
            "Resolving deltas: 100% (175/175), done.\n",
            "Collecting fire (from -r /content/mistral-finetune/requirements.txt (line 1))\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 2)) (0.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 3)) (6.0.1)\n",
            "Collecting mistral-common>=1.1.0 (from -r /content/mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading mistral_common-1.2.1-py3-none-any.whl (704 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.9/704.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 5)) (0.4.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 6)) (2.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 7)) (4.66.4)\n",
            "Collecting torch==2.2 (from -r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2 (from -r /content/mistral-finetune/requirements.txt (line 10))\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers==0.0.24 (from -r /content/mistral-finetune/requirements.txt (line 11))\n",
            "  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.2/218.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.24->-r /content/mistral-finetune/requirements.txt (line 11)) (1.25.2)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r /content/mistral-finetune/requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r /content/mistral-finetune/requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->-r /content/mistral-finetune/requirements.txt (line 2)) (0.16)\n",
            "Collecting jsonschema==4.21.1 (from mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==2.6.1 (from mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (0.1.99)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (0.18.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.6.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (0.7.0)\n",
            "Collecting pydantic-core==2.16.2 (from pydantic==2.6.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading pydantic_core-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=b5e10c91ddbb1d3817ec3addad492f3cfb24161b27870c0a6efb3a9264ef7461\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: triton, pydantic-core, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, jsonschema, torch, mistral-common, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.18.4\n",
            "    Uninstalling pydantic_core-2.18.4:\n",
            "      Successfully uninstalled pydantic_core-2.18.4\n"
          ]
        }
      ],
      "source": [
        "# Installation\n",
        "!git clone https://github.com/mistralai/mistral-finetune.git\n",
        "!pip install -r /content/mistral-finetune/requirements.txt\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Set up model and tokenizer\n",
        "model_name = \"Mistral-7B-v0.3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up sentence embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate fake data\n",
        "def generate_fake_data(num_samples):\n",
        "    triggers = [\"frustrated customer\", \"priority client\", \"escalate issue\"]\n",
        "    behaviors = [\"Respond empathetically and de-escalate\", \"Provide prompt and personalized assistance\", \"Escalate to supervisor for immediate resolution\"]\n",
        "\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        trigger = random.choice(triggers)\n",
        "        behavior = random.choice(behaviors)\n",
        "        instruction = f\"Customer reports {random.choice(['account locked', 'payment declined', 'order delayed'])} unexpectedly\"\n",
        "        response = f\"I apologize for the inconvenience this has caused. Let's work together to resolve this issue promptly. Could you please provide more details about when you first encountered the problem with your {random.choice(['account', 'payment', 'order'])}?\"\n",
        "\n",
        "        data.append({\n",
        "            \"trigger\": trigger,\n",
        "            \"instruction\": instruction,\n",
        "            \"target_behavior\": behavior,\n",
        "            \"response\": response\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "# Create dataset\n",
        "class CustomerServiceDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        trigger = item[\"trigger\"]\n",
        "        instruction = item[\"instruction\"]\n",
        "        behavior = item[\"target_behavior\"]\n",
        "        response = item[\"response\"]\n",
        "\n",
        "        input_text = f\"<trigger>{trigger}</trigger><instruction>{instruction}</instruction><target_behavior>{behavior}</target_behavior><response>{response}</response>\"\n",
        "        input_ids = tokenizer.encode(input_text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"target_behavior\": behavior,\n",
        "            \"response\": response\n",
        "        }\n",
        "\n",
        "# Set up data loader\n",
        "train_data = generate_fake_data(1000)\n",
        "train_dataset = CustomerServiceDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Define custom loss function\n",
        "def custom_loss_function(outputs, labels, target_behaviors, target_responses, w1=0.7, w2=0.15, w3=0.15):\n",
        "    response_loss = torch.nn.CrossEntropyLoss()(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
        "\n",
        "    generated_responses = tokenizer.batch_decode(outputs.logits.argmax(dim=-1))\n",
        "    generated_embeddings = embedding_model.encode(generated_responses)\n",
        "    behavior_embeddings = embedding_model.encode(target_behaviors)\n",
        "    response_embeddings = embedding_model.encode(target_responses)\n",
        "\n",
        "    behavior_loss = 1 - torch.mean(torch.cosine_similarity(generated_embeddings, behavior_embeddings))\n",
        "    example_loss = 1 - torch.mean(torch.cosine_similarity(generated_embeddings, response_embeddings))\n",
        "\n",
        "    total_loss = w1 * response_loss + w2 * behavior_loss + w3 * example_loss\n",
        "    return total_loss\n",
        "\n",
        "# Training loop\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].squeeze(1).to(model.device)\n",
        "        labels = input_ids.clone()\n",
        "        target_behaviors = batch[\"target_behavior\"]\n",
        "        target_responses = batch[\"response\"]\n",
        "\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = custom_loss_function(outputs, labels, target_behaviors, target_responses)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mIgfpAzzqvbo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}